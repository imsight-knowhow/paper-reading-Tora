\section{Conclusion}
This paper introduces Tora, the first trajectory-oriented Diffusion Transformer framework for video generation. 
Tora effectively encodes arbitrary trajectories into spacetime motion patches, which align well with the scaling properties of DiT, thereby enabling more realistic simulations of physical world movements. By employing a two-stage training process, Tora achieves motion-controllable video generation across a wide range of durations, aspect ratios, and resolutions. Remarkably, it can generate high-quality videos that adhere to specified trajectories, producing up to 204 frames at 720p resolution. This capability underscores Tora's versatility and robustness in handling diverse motion patterns while maintaining high visual fidelity. We hope our work provides a strong baseline for future research in motion-guided Diffusion Transformer methods.

% \section*{Acknowledgments}
