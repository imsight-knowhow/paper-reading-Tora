\section{Introduction}


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.94\textwidth]{images/f2.pdf}
    \caption{
         % More generated samples. Tora accommodates diverse conditions such as text-only inputs, single starting frames, and combinations of initial and final frames (as illustrated in the fifth row). It effectively manages multiple trajectories, allowing for precise manipulation of several objects. Furthermore, Tora supports video generation across different aspect ratios, resolutions, and durations, ensuring flexible content creation.
         More generated samples. Tora effectively manages trajectories to precisely manipulate various objects and backgrounds. In the realm of image-to-video synthesis, it can craft dynamic camera movements in accordance with textual descriptions and the designated starting trajectory points, such as common backgrounds.  Furthermore, Tora supports video generation across different aspect ratios, resolutions, and durations, ensuring flexible content creation. 
    }
    \label{fig:2}   
\end{figure*}

Diffusion models~\cite{Dhariwal2021DiffusionMB, ramesh2022hierarchical} have demonstrated their capability to generate diverse and high-quality images or videos. Previously, video diffusion models~\cite{ho2022video, DBLP:journals/corr/abs-2311-15127,DBLP:journals/corr/abs-2311-04145} predominantly employed UNet-based architectures~\cite{ronneberger2015u}, focusing primarily on synthesizing videos of limited duration, typically around two seconds, and were constrained to fixed resolutions and aspect ratios. Recently, Sora~\cite{sora2024}, a text-to-video generation model leveraging Diffusion Transformer~(DiT)~\cite{peebles2023scalable}, has showcased video generation capabilities that significantly outstrip current state-of-the-art methods. Sora excels not only in the production of high-quality videos ranging from 10 to 60 seconds, but also distinguishes itself through its capacity to handle diverse resolutions, various aspect ratios, adherence to the laws of actual physics.

Video generation requires consistent motion across image sequences, underscoring the importance of motion control. Previous works, such as VideoComposer~\cite{wang2023videocomposer} and DragNUWA~\cite{yin2023dragnuwa}, have implemented generalized motion manipulation through motion vectors and trajectories. Building on this foundation, MotionCtrl~\cite{wang2024motionctrl} innovates by independently managing camera and object motions, thereby expanding the diversity of achievable motion patterns. Despite their promising controllable motion quality, UNet-based methods are restricted to generating videos of only 16 frames at a fixed, lower resolution. This limitation hinders the smooth portrayal of motion, particularly during significant positional shifts in the provided trajectory, leading to distortion and unnatural movements, such as parallel drifting, which diverge from real-world dynamics. Consequently, there is an urgent need for a model capable of producing longer videos with robust motion control and detailed physical representations.

%To address these challenges, we introduce Toraâ€”a pioneering DiT model that simultaneously integrates text, image, and trajectory, ensuring scalable video generation and robust motion control. 
To address these challenges, we present Tora, the first DiT model that simultaneously integrates text, images, and trajectories, enabling scalable video generation with robust motion control.
Notably, our work adopts OpenSora~\cite{OpenSora}, an open-source version of Sora, as the foundational DiT model. To align motion control with the scalability of the DiT framework, we propose two novel modules: the Trajectory Extractor (TE), which converts arbitrary trajectories into hierarchical spacetime motion patches, and the Motion-guidance Fuser (MGF), designed to seamlessly integrate these patches within the stacked DiT blocks.
%To achieve motion control that aligns with the scalability of the DiT framework, we propose two innovative modules. The first, the Trajectory Extractor (TE), transforms arbitrary trajectories into hierarchical spacetime motion patches. The second, the Motion-guidance Fuser (MGF), is meticulously crafted to integrate these motion patches within the stacked architecture of the DiT blocks. To facilitate motion control, we design two innovative modules: the Trajectory Extractor (TE), which transforms arbitrary trajectories into hierarchical spacetime motion patches, and the Motion-guidance Fuser (MGF), meticulously crafted to integrate these motion patches within the stacked architecture of DiT blocks.
More specifically, TE initially converts positional displacements along trajectory into the RGB domain via flow visualization techniques. These visualized displacements undergo Gaussian filtering to mitigate scattered issues. Subsequently, a 3D Variational Autoencoder (VAE)~\cite{kingma2013auto} encodes trajectory visualizations into spacetime motion latents, which share the same latent space with video patches. The motion latents are then decomposed into multiple levels of motion conditions via stacked lightweight modules. Our VAE architecture is inspired by MAGVIT-v2~\cite{DBLP:journals/corr/abs-2310-05737} but simplified by omitting codebook dependencies. The MGF integrates adaptive normalization layers~\cite{DBLP:conf/aaai/PerezSVDC18} to infuse multi-level motion conditions into the corresponding DiT blocks. We explored various adaptations of transformer blocks including adaptive layer normalization, cross-attention, and extra channel connections to inject the motion conditions. Among these, adaptive layer normalization emerged as the most effective to generate consistent videos following the trajectory.

During training, we adapt OpenSora's workflow to generate high-quality video-text pairs and utilize an optical flow estimator~\cite{DBLP:journals/pami/XuZCRYTG23} for trajectory extraction. We also integrate a motion segmentor~\cite{DBLP:conf/eccv/ZhaoLGWL22} with a camera detector\footnote{https://github.com/antiboredom/camera-motion-detector} to filter out instances dominated by camera motion, thereby improving our tracking of object trajectories. This careful selection process results in a dataset of high-quality videos with consistent motion. With an adapter-like strategy~\cite{DBLP:conf/aaai/MouWXW0QS24}, we solely train the temporal blocks, together with the TE and MGF. This strategy seamlessly integrates DiT's inherent generative knowledge with external motion signals.

%To train Tora, annotated videos with captions and movement trajectories are essential. We adapt OpenSora's workflow to transform raw videos into high-quality video-text pairs and leverage an optical flow estimator~\cite{DBLP:journals/pami/XuZCRYTG23} for trajectory extraction. Significantly, we combine a motion segmentor ~\cite{DBLP:conf/eccv/ZhaoLGWL22} with a camera detector \footnote{https://github.com/antiboredom/camera-motion-detector} to filter out instances that primarily contain camera movement. This strategic approach improves our ability to accurately follow the trajectory of specific objects within videos. As a result, this careful selection process leads to the creation of a dataset containing 630k high-quality videos with consistent motion. With an adapter-like strategy~\cite{DBLP:conf/aaai/MouWXW0QS24}, we solely train the temporal blocks, together with the TE and MGF. This strategy seamlessly integrates DiT's inherent generative knowledge with external motion signals.

The main contributions of our work are as follows:

\begin{itemize}
\item %We introduce Tora, the first trajectory-oriented DiT model for scalable video generation with strong motion guidance. As illustrated in Figure~\ref{fig:2}, Tora seamlessly integrates various text, visual and trajectory instructions, enabling the creation of motion-manipulable videos.
We introduce Tora, the first trajectory-oriented DiT model for video generation. As illustrated in Figure~\ref{fig:2}, Tora enables the creation of motion-manipulable videos with varying aspect ratios, extending up to 204 frames and 720p resolution.

%Tora seamlessly integrates a broad range of visual and trajectory instructions, thereby proficiently enabling the creation of motion-manipulable videos.
\item %We propose a novel trajectory extractor and a motion-guidance fusion mechanism to facilitate motion control that aligns with the scalability of DiT. Additionally, we ablate several architecture choices and offer empirical baselines for future research.
We propose a novel trajectory extractor and a motion-guidance fusion mechanism to facilitate motion control that aligns with the scalability of DiT. Additionally, we ablate several architectural choices and scaling capabilities to offer empirical baselines for future research.

\item %Experiments demonstrate that Tora is capable of generating 720p resolution videos with varying aspect ratios, extending up to 204 frames, all guided by the specified trajectories. Furthermore, it demonstrates superiority in simulating movements within the physical world.
Experiments demonstrate that Tora achieves state-of-the-art accuracy in controlling object motions. Furthermore, it demonstrates superiority in simulating movements within the physical world.
\end{itemize}
